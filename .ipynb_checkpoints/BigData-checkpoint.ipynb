{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup and others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Change the jupyter theme <br>\n",
    "`!jt -t grade3`\n",
    "\n",
    "2. Setting up a new environment variable(windows)\n",
    "    - New System variable\n",
    "    - Variable name: e.g. SPARK_HOME\n",
    "    - Variable value: e.g. C:\\spark1.6\n",
    "    - Edit Path -> New -> %SPARK_HOME%\\bin\n",
    "\n",
    "3. Solve hive's metastore issue\n",
    "    - mysql -u root -p\n",
    "    - create database metastore;\n",
    "    - SET GLOBAL time_zone = '+3:00';\n",
    "    - schematool -initSchema -dbType mysql\n",
    "\n",
    "4. Running argv script from PyCharm\n",
    "    - Run\n",
    "    - Script path\n",
    "    - the parameters\n",
    "\n",
    "5. Setting up PyCharm with Spark\n",
    "    - Settings\n",
    "    - Project -> Project Structure\n",
    "    - Add Content Root -> C:\\spark\\python\n",
    "    - Add Content Root -> C:\\spark\\python\\lib\\py4j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Properties files\n",
    "    - `/hadoop/etc/hadoop/core-site.xml`\n",
    "    - `/hadoop/etc/hadoop/hdfs-site.xml`\n",
    "\n",
    "2. Properties\n",
    "    - `fs.defaultFS`\n",
    "    - `dfs.blocksize` -> the blocksize into which the files will be divided\n",
    "    - `dfs.replication` -> the number of copies for the files\n",
    "\n",
    "3. Web interface\n",
    "    - `http://localhost:9870`\n",
    "\n",
    "4. Commands\n",
    "    - `ls -ltr|grep dbratu` - searching for some specific pattern\n",
    "    - `export SPARK_MAJOR_VERSION=2` - run spark 2.x\n",
    "    - `hostname -f` - check the hostname\n",
    "    - `winutils chmod 777 /tmp/hive` - solve spark-shell error\n",
    "    - `tar xzvf <file_path>` - eXtract, uncompress, Verbose, the entire File\n",
    "    - `du -sh` - check the files size\n",
    "    - `wc -l /Users/dbratu/Documents/big_data/data-master/retail_db/*/*` - check number of records\n",
    "    - `hadoop fs` - the command line interface\n",
    "    - `hadoop fs -ls` - list the files\n",
    "    - `hadoop fs -tail <path>` - show the last lines of a file\n",
    "    - `hadoop fs -ls -R` - including subdirectories\n",
    "    - `hadoop fs -du -s -h` - check the file size\n",
    "    - `hadoop fsck /user/dbratu/data  -files -locations` - locations of the files\n",
    "    - `hadoop fs -get <source> <destination>`\n",
    "    - `hadoop fs -copyToLocal <source> <destination>`\n",
    "\n",
    "5. Yarn (Yet Another Resource Negotiator)\n",
    "    - Properties files\n",
    "        - `/hadoop/etc/hadoop/yarn-site.xml`\n",
    "        - `/spark/conf/spark-env.sh`\n",
    "    - Web interface\n",
    "        - `http://localhost:8088/cluster`\n",
    "\n",
    "6. Compression\n",
    "    - go to /etc/hadoop/conf\n",
    "    - view core-site.xml\n",
    "    - search for \"codecs\"\n",
    "    - org.apache.hadoop.io.compress.GzipCodec\n",
    "    - org.apache.hadoop.io.compress.DefaultCodec\n",
    "    - org.apache.hadoop.io.compress.SnappyCodec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information\n",
    "- spark.apache.org\n",
    "- spark2 uses dataframes more\n",
    "- df is rdd with structure\n",
    "- Start spark-sql session:<br>\n",
    "`spark-sql --conf spark.ui.port=4040`\n",
    "- Execution frameworks:\n",
    "    - hive: mapreduce\n",
    "    - spark-sql: spark\n",
    "- In hive context there is no: `hadoop fs`, there is `dfs`\n",
    "\n",
    "- Metastore<br>\n",
    "`set hive.metastore.warehouse.dir`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI commands\n",
    "- `spark-submit`: see parameters\n",
    "- `pyspark`\n",
    "    - `--master yarn`\n",
    "    - `--conf spark.ui.port=4040`: the web interface is at http://localhost:4040/ (default)\n",
    "    - `--num-executors 2 `\n",
    "    - `--executor-cores 2`\n",
    "    - `--executor-memory 512M`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Port Numbers:\n",
    "NameNode – Port 50070 <br>\n",
    "Task Tracker – Port 50060<br>\n",
    "Job Tracker – Port 50030<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config\n",
    "- Force spark to use x threads<br>\n",
    "`spark.conf.set('spark.sql.shuffle.partitions', 'x')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and transformations (RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = sc.textFile(\"/path/\", minPartitions=18)` # the number of splits of file, tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create rdd from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_list = [1, 2, 3, 4]\n",
    "rdd = sc.parallelize(rdd_list)\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading different file formats using sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.read.text(\"/path/\")\n",
    "sqlContext.read.json(\"/path/\")\n",
    "sqlContext.read.orc(\"/path/\")\n",
    "sqlContext.read.parquet(\"/path/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_map = rdd.map(lambda x: x.split(\",\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words = ['Dragos Bratu Florian', 'Bratu Dragos', \"Dragos Meltzer\"]\n",
    "list_rdd = sc.parallelize(list_of_words)\n",
    "words = list_rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(words.collect())` # ['Dragos', 'Bratu', 'Florian', 'Bratu', 'Dragos', 'Dragos', 'Meltzer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\")\n",
    "filtered = orders.filter(lambda x: x.split(\",\")[3] in [\"CLOSED\", \"COMPLETE\"])\n",
    "print(*orders.take(3), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join\n",
    "(k, v) join (k, w) -> (k, (v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\")\n",
    "order_items = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/order_items\")\n",
    "products = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/products\")\n",
    "\n",
    "orders_map = orders.map(lambda x: (int(x.split(\",\")[0]), x.split(\",\")[1]))\n",
    "order_items_map = order_items.map(lambda x: (int(x.split(\",\")[1]), float(x.split(\",\")[4])))\n",
    "\n",
    "orders_join = orders_map.join(order_items_map)\n",
    "print(*orders_join.take(3), sep=\"\\n\")\n",
    "# (2, ('2013-07-25 00:00:00.0', 199.99))\n",
    "# (2, ('2013-07-25 00:00:00.0', 250.0))\n",
    "# (2, ('2013-07-25 00:00:00.0', 129.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_LO_join = orders_map.leftOuterJoin(order_items_map)\n",
    "orders_RO_join = orders_map.rightOuterJoin(order_items_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = order_items.filter(lambda oi: int(oi.split(\",\")[1]) == 2).map(lambda x: float(x.split(\",\")[4])).reduce(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_status = orders.map(lambda x: (x.split(\",\")[-1], 1))\n",
    "for item in order_status.countByKey().items():\n",
    "    print(*item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combiner (using threads)\n",
    "- groupByKey() - doesn't use combiner\n",
    "- reduceByKey() - use combiner : when there is only one function for combiner and reduce\n",
    "- aggregateByKey() - use combiner : when there are two functions for combiner and reduce\n",
    "    - (1, 1..1000) - sum(1, 2, 3, 4, 5, ..., 1000)\n",
    "    - (1, 1..1000) - sum(sum(1, 250), sum(251, 500), sum(501, 750), sum(751, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = order_items.map(lambda x: (int(x.split(\",\")[1]), float(x.split(\",\")[4]))).groupByKey()\n",
    "oi_gr = oi_map.groupByKey() \n",
    "# (1, <pyspark.resultiterable.ResultIterable object at 0x7fd5c25283a0>)\n",
    "\n",
    "res = res.collect()\n",
    "for i in range(3):\n",
    "    print(res[i][0], list(res[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sorted inside flatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_sort = oi_gr.flatMap(lambda oi: sorted(oi[1], key=lambda k: float(k.split(\",\")[4]), reverse=True))\n",
    "print(*oi_sort.take(10), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_map = order_items.map(lambda x: (int(x.split(\",\")[1]), float(x.split(\",\")[4])))\n",
    "# (1, 299.98)\n",
    "oi_rdb = oi_map.reduceByKey(lambda x, y: x + y)\n",
    "print(*oi_rdb.take(3), sep=\"\\n\")`\n",
    "# (1, 299.98)\n",
    "# (2, 579.98)\n",
    "# (4, 699.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregateByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi_map = order_items.map(lambda x: (int(x.split(\",\")[1]), float(x.split(\",\")[4])))\n",
    "# (1, 299.98)\n",
    "oi_agg = oi_map.aggregateByKey((0.0, 0),\n",
    "                               lambda x, y: (x[0] + y, x[1] + 1),\n",
    "                               lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "\n",
    "print(*oi_agg.take(10), sep=\"\\n\")\n",
    "# (1, (299.98, 1))\n",
    "# (2, (579.98, 3))\n",
    "# (4, (699.85, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_map = products.filter(lambda z: z.split(\",\")[4] != \"\").map(lambda p: (-float(p.split(\",\")[4]), p))`\n",
    "# (59.98, '1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy')\n",
    "\n",
    "prod_sort = prod_map.sortByKey()\n",
    "# (-1999.99, '208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical')\n",
    "# (-1799.99, '66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill')\n",
    "    \n",
    "prod_sort_map = prod_sort.map(lambda x: x[1])\n",
    "# 208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical\n",
    "# 66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### double sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_map = products.filter(lambda z: z.split(\",\")[4] != \"\").map(lambda p: ((int(p.split(\",\")[1]), -float(p.split(\",\")[4])), p))`\n",
    "# ((2, -59.98), '1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy'\n",
    "# ((2, -59.98), '1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy'\n",
    "# ((2, -59.98), '1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy'\n",
    "# ((2, -59.98), '1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy')\n",
    "\n",
    "prod_sort = prod_map.sortByKey(False)\n",
    "# ((2, -299.99), '16,2,Riddell Youth 360 Custom Football Helmet,,299.99,http://images.acmesports.sports/Riddell+Youth+360+Custom+Football+Helmet')\n",
    "# ((2, -209.99), '11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set')\n",
    "# ((2, -199.99), '5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking\n",
    "1. takeOrdered() - by default it sorts ascending\n",
    "2. top() - by default it sorts descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_map = products.filter(lambda z: z.split(\",\")[4] != \"\")\n",
    "top5 = prod_map.takeOrdered(5, key=lambda x: -float(x.split(\",\")[4]))\n",
    "# or\n",
    "top5 = prod_map.top(5, key=lambda x: float(x.split(\",\")[4]))\n",
    "# 208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical\n",
    "# 66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill\n",
    "# 199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 3 per each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_fmg = products.filter(lambda z: z.split(\",\")[4] != \"\").\\\n",
    "    map(lambda p: (int(p.split(\",\")[1]), p)).\\\n",
    "    groupByKey()\n",
    "# (2, <pyspark.resultiterable.ResultIterable object at 0x7fad3bae9640>)\n",
    "# (3, <pyspark.resultiterable.ResultIterable object at 0x7fad3bae94f0>)\n",
    "# (4, <pyspark.resultiterable.ResultIterable object at 0x7fad3bae94c0>)\n",
    "\n",
    "top3 = prod_fmg.flatMap(lambda x: sorted(x[1], key=lambda y: float(y.split(\",\")[4]), reverse=True)[:3])\n",
    "# 16,2,Riddell Youth 360 Custom Football Helmet,,299.99,http://images.acmesports.sports/Riddell+Youth+360+Custom+Football+Helmet\n",
    "# 11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set\n",
    "# 5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n",
    "# 40,3,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy\n",
    "# 32,3,PUMA Men's evoPOWER 1 Tricks FG Soccer Cleat,,189.99,http://images.acmesports.sports/PUMA+Men%27s+evoPOWER+1+Tricks+FG+Soccer+Cleat\n",
    "# 35,3,adidas Brazuca 2014 Official Match Ball,,159.99,http://images.acmesports.sports/adidas+Brazuca+2014+Official+Match+Ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take top3 different prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_fmg = products.filter(lambda z: z.split(\",\")[4] != \"\").\\\n",
    "    map(lambda p: (int(p.split(\",\")[1]), p)).groupByKey()\n",
    "# (2, <pyspark.resultiterable.ResultIterable object at 0x7fad3bae9640>)\n",
    "\n",
    "def take_top(grouped_products, top_N):\n",
    "    sorted_prices = sorted(grouped_products[1], key=lambda x: float(x.split(\",\")[4]), reverse=True)\n",
    "    list_prices = map(lambda x: float(x.split(\",\")[4]), sorted_prices)\n",
    "    top_prices = sorted(set(list_prices), reverse=True)[:top_N]\n",
    "    from itertools import takewhile\n",
    "    return takewhile(lambda y: float(y.split(\",\")[4]) in top_prices, sorted_prices)\n",
    "\n",
    "result = prod_fmg.flatMap(lambda x: take_top(x, 3))\n",
    "# 16,2,Riddell Youth 360 Custom Football Helmet,,299.99,http://images.acmesports.sports/Riddell+Youth+360+Custom+Football+Helmet\n",
    "# 11,2,Fitness Gear 300 lb Olympic Weight Set,,209.99,http://images.acmesports.sports/Fitness+Gear+300+lb+Olympic+Weight+Set\n",
    "# 5,2,Riddell Youth Revolution Speed Custom Footbal,,199.99,http://images.acmesports.sports/Riddell+Youth+Revolution+Speed+Custom+Football+Helmet\n",
    "# 14,2,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy\n",
    "# 40,3,Quik Shade Summit SX170 10 FT. x 10 FT. Canop,,199.99,http://images.acmesports.sports/Quik+Shade+Summit+SX170+10+FT.+x+10+FT.+Canopy\n",
    "# 32,3,PUMA Men's evoPOWER 1 Tricks FG Soccer Cleat,,189.99,http://images.acmesports.sports/PUMA+Men%27s+evoPOWER+1+Tricks+FG+Soccer+Cleat\n",
    "# 35,3,adidas Brazuca 2014 Official Match Ball,,159.99,http://images.acmesports.sports/adidas+Brazuca+2014+Official+Match+Ball\n",
    "# 48,3,adidas Brazuca Final Rio Official Match Ball,,159.99,http://images.acmesports.sports/adidas+Brazuca+Final+Rio+Official+Match+Ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders201312 = orders.filter(lambda o: o.split(\",\")[1][:7] == \"2013-12\").\\\n",
    "    map(lambda x: (int(x.split(\",\")[0]), x))\n",
    "orders201401 = orders.filter(lambda o: o.split(\",\")[1][:7] == \"2014-01\").\\\n",
    "    map(lambda x: (int(x.split(\",\")[0]), x))\n",
    "# (25876, '25876,2014-01-01 00:00:00.0,3414,PENDING_PAYMENT')\n",
    "\n",
    "order_items_m = order_items.map(lambda oi: (int(oi.split(\",\")[1]), oi))\n",
    "oi2013 = orders201312.join(order_items_m).map(lambda x: x[1][1])\n",
    "oi2014 = orders201401.join(order_items_m).map(lambda x: x[1][1])\n",
    "# 52252,20916,957,1,299.98,299.98\n",
    "# 52253,20916,365,2,119.98,59.99\n",
    "# 52254,20916,897,5,124.95,24.99\n",
    "\n",
    "prod_13 = oi2013.map(lambda x: int(x.split(\",\")[2]))\n",
    "prod_14 = oi2014.map(lambda x: int(x.split(\",\")[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allprod = prod_13.union(prod_14) # not giving distinct records\n",
    "print(allprod.count()) # 29395\n",
    "allprod = prod_13.union(prod_14).distinct() # not giving distinct records\n",
    "print(allprod.count()) # 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inters = prod_13.intersection(prod_14) # distinct is implicit\n",
    "print(*inters.take(10), sep=\"\\n\")\n",
    "print(inters.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only13 = prod_13.subtract(prod_14) # distinct not implicit\n",
    "only14 = prod_14.subtract(prod_13)\n",
    "print(only13.collect()) # [127, 127, 127]\n",
    "print(only14.collect()) # [58, 58]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allprod.coalesce(2).saveAsTextFile(\"/Users/dbratu/Documents/big_data/all2\",\n",
    "                       compressionCodecClass='org.apache.hadoop.io.compress.SnappyCodec')\n",
    "# this path doesn't have to exist\n",
    "# only 2 files as output\n",
    "# compressed as Snappy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### other formats\n",
    "first the rdd has to be converted into a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.toDF(schema=[\"col1\", \"col2\"])\n",
    "df.save(\"<path>\", \"json\")\n",
    "\n",
    "df.write.json(\"<path>\")\n",
    "df.write.json(\"<path>\", \"gzip\") # with compression\n",
    "df.write.json(\"<path>\", \"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark\n",
    "    --master yarn\n",
    "    --conf spark.ui.port=4040 \n",
    "    --num-executors 2 \n",
    "    --executor-memory 512M\n",
    "    --packages com.databricks:spark-avro_2.10:2.0.1\n",
    "or\n",
    "    --jars <path_to_jar>\n",
    "\n",
    "data_df.save(\"<path>\", \"com.databricks:spark-avro\")\n",
    "\n",
    "# check data\n",
    "sqlContext.load(\"<path>\", \"com.databricks:spark-avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions and transformations (DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"DailyRevenue\").\\\n",
    "    master(props.get(env, \"executionMode\")).\\\n",
    "    getOrCreate()\n",
    "# or\n",
    "sc = SparkContext(master=\"local\", appName=\"Spark Demo\")\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.functions import split, explode\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"DailyRevenue\").\\\n",
    "    master(\"local\").\\\n",
    "    getOrCreate()\n",
    "data = spark.read.text(\"/Users/dbratu/Documents/notite.sql\")\n",
    "wc = data.select(explode(split(data.value, \" \")).alias(\"words\")).groupBy(\"words\").count()\n",
    "wc.write.csv(\"/Users/dbratu/Documents/wc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF = spark.\n",
    "    read.\n",
    "    csv(\"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\")\n",
    "\n",
    "ordersFRM = spark.read.\n",
    "    format('csv').\n",
    "    option('sep', ',').\n",
    "    schema('order_id int, order_date string, order_customer_id int, order_status string').\n",
    "    load('/Users/dbratu/Documents/big_data/data-master/retail_db/orders')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.take()\n",
    "ordersDF.select(\"_c0\", \"_c1\", \"_c2\", \"_c3\").show(5)\n",
    "# +---+--------------------+-----+---------------+\n",
    "# |_c0|                 _c1|  _c2|            _c3|\n",
    "# +---+--------------------+-----+---------------+\n",
    "# |  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
    "# |  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
    "# |  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
    "# |  4|2013-07-25 00:00:...| 8827|         CLOSED|\n",
    "# |  5|2013-07-25 00:00:...|11318|       COMPLETE|\n",
    "# +---+--------------------+-----+---------------+`\n",
    "\n",
    "ordersDF.select(\"_c0\", \"_c1\", \"_c2\", \"_c3\").show(5, False)\n",
    "\n",
    "# +---+---------------------+-----+---------------+\n",
    "# |_c0|_c1                  |_c2  |_c3            |\n",
    "# +---+---------------------+-----+---------------+\n",
    "# |1  |2013-07-25 00:00:00.0|11599|CLOSED         |\n",
    "# |2  |2013-07-25 00:00:00.0|256  |PENDING_PAYMENT|\n",
    "# |3  |2013-07-25 00:00:00.0|12111|COMPLETE       |\n",
    "# |4  |2013-07-25 00:00:00.0|8827 |CLOSED         |\n",
    "# |5  |2013-07-25 00:00:00.0|11318|COMPLETE       |\n",
    "# +---+---------------------+-----+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.describe().show()\n",
    "# +-------+------------------+--------------------+-----------------+---------------+\n",
    "# |summary|               _c0|                 _c1|              _c2|            _c3|\n",
    "# +-------+------------------+--------------------+-----------------+---------------+\n",
    "# |  count|             68883|               68883|            68883|          68883|\n",
    "# |   mean|           34442.0|                null|6216.571098819738|           null|\n",
    "# | stddev|19884.953633337947|                null|3586.205241263963|           null|\n",
    "# |    min|                 1|2013-07-25 00:00:...|                1|       CANCELED|\n",
    "# |    max|              9999|2014-07-24 00:00:...|             9999|SUSPECTED_FRAUD|\n",
    "# +-------+------------------+--------------------+-----------------+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.createTempView('orders')\n",
    "spark.sql(\"select * from orders limit 3\").show()\n",
    "\n",
    "# +---+--------------------+-----+---------------+\n",
    "# |_c0|                 _c1|  _c2|            _c3|\n",
    "# +---+--------------------+-----+---------------+\n",
    "# |  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
    "# |  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
    "# |  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
    "# +---+--------------------+-----+---------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersTB = spark.read.table(\"dbratu.orders\")\n",
    "spark.sql(\"select * from dbratu.orders limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "ordersDF.select(substring(ordersDF.order_date, 0, 7).alias(\"Dra\")).show(5)\n",
    "# +-------+\n",
    "# |    Dra|\n",
    "# +-------+\n",
    "# |2013-07|\n",
    "# |2013-07|\n",
    "# |2013-07|\n",
    "# |2013-07|\n",
    "# |2013-07|\n",
    "# +-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a new column worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.withColumn(\"new_col\", substring(ordersDF.order_date, 1, 7)).show(2)\n",
    "# +--------+--------------------+-----------------+---------------+-------+\n",
    "# |order_id|          order_date|order_customer_id|   order_status|new_col|\n",
    "# +--------+--------------------+-----------------+---------------+-------+\n",
    "# |       1|2013-07-25 00:00:...|            11599|         CLOSED|2013-07|\n",
    "# |       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|2013-07|\n",
    "# +--------+--------------------+-----------------+---------------+-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SelectExpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.select(substring(ordersDF.order_date, 0, 7).alias(\"Dra\")).show(3)\n",
    "ordersDF.selectExpr(\"substring(order_date, 1, 7) as order_month\").show(3)\n",
    "# +-----------+\n",
    "# |order_month|\n",
    "# +-----------+\n",
    "# |    2013-07|\n",
    "# |    2013-07|\n",
    "# |    2013-07|\n",
    "# +-----------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.filter(ordersDF.order_status == \"COMPLETE\").show(5)\n",
    "# or\n",
    "ordersDF.filter(\"order_status = 'COMPLETE'\").show(5)\n",
    "# +--------+--------------------+-----------------+------------+\n",
    "# |order_id|          order_date|order_customer_id|order_status|\n",
    "# +--------+--------------------+-----------------+------------+\n",
    "# |       3|2013-07-25 00:00:...|            12111|    COMPLETE|\n",
    "# |       5|2013-07-25 00:00:...|            11318|    COMPLETE|\n",
    "# |       6|2013-07-25 00:00:...|             7130|    COMPLETE|\n",
    "# |       7|2013-07-25 00:00:...|             4530|    COMPLETE|\n",
    "# |      15|2013-07-25 00:00:...|             2568|    COMPLETE|\n",
    "# +--------+--------------------+-----------------+------------+\n",
    "\n",
    "\n",
    "# or |\n",
    "ordersDF.filter((ordersDF.order_status == \"COMPLETE\") | (ordersDF.order_status == \"CLOSED\")).show(5)\n",
    "# isin()\n",
    "ordersDF.filter(ordersDF.order_status.isin(\"COMPLETE\", \"CLOSED\")).show(5)\n",
    "\n",
    "# slq in\n",
    "ordersDF.filter(\"order_status in ('COMPLETE', 'CLOSED')\").show(5)\n",
    "\n",
    "# like in\n",
    "ordersDF.filter((ordersDF.order_status.isin(\"COMPLETE\", \"CLOSED\")) &\n",
    "                (ordersDF.order_date.like(\"2013-08%\"))).show(5)\n",
    "\n",
    "# like in sql\n",
    "ordersDF.filter(\"order_status in ('COMPLETE', 'CLOSED') and order_date like '2013-08%'\").show(5)\n",
    "\n",
    "# date_format\n",
    "ordersDF.filter(date_format(ordersDF.order_date, 'dd') == '01').select('order_date').distinct().show(100, False)\n",
    "ordersDF.filter(\"date_format(order_date, 'dd') = '01'\").select('order_date').distinct().show(100, False)\n",
    "\n",
    "# +---------------------+\n",
    "# |order_date           |\n",
    "# +---------------------+\n",
    "# |2014-07-01 00:00:00.0|\n",
    "# |2013-08-01 00:00:00.0|\n",
    "# |2014-03-01 00:00:00.0|\n",
    "# |2014-01-01 00:00:00.0|\n",
    "# |2014-05-01 00:00:00.0|\n",
    "# |2013-11-01 00:00:00.0|\n",
    "# |2014-02-01 00:00:00.0|\n",
    "# |2013-10-01 00:00:00.0|\n",
    "# |2013-12-01 00:00:00.0|\n",
    "# |2014-04-01 00:00:00.0|\n",
    "# |2013-09-01 00:00:00.0|\n",
    "# |2014-06-01 00:00:00.0|\n",
    "# +---------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF_f = ordersDF.where(ordersDF.order_status.isin(\"CLOSED\", \"COMPLETE\"))\n",
    "ordersDF_j = ordersDF_f.join(order_itemsDF, ordersDF_f.order_id == order_itemsDF.order_item_order_id, \"inner\")\n",
    "\n",
    "# data in the first table but not in the second one\n",
    "ordersDF.join(order_itemsDF, ordersDF.order_id == order_itemsDF.order_item_order_id, 'left').where(\"order_item_order_id is null\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "ordersDF.select(countDistinct(ordersDF.order_status).alias('Counter')).show()\n",
    "# +-------+\n",
    "# |Counter|\n",
    "# +-------+\n",
    "# |      9|\n",
    "# +-------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no alias prossible\n",
    "order_itemsDF.groupBy(\"order_item_order_id\").sum(\"order_item_subtotal\").show()\n",
    "\n",
    "# alias possible: agg\n",
    "order_itemsDF.groupBy(\"order_item_order_id\").agg(sum(\"order_item_subtotal\").alias(\"Dragos\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.groupBy(\"order_status\").agg(count(\"order_status\").alias(\"Counter\")).show()\n",
    "# +---------------+-------+\n",
    "# |   order_status|Counter|\n",
    "# +---------------+-------+\n",
    "# |PENDING_PAYMENT|  15030|\n",
    "# |       COMPLETE|  22899|\n",
    "# |        ON_HOLD|   3798|\n",
    "# | PAYMENT_REVIEW|    729|\n",
    "# |     PROCESSING|   8275|\n",
    "# |         CLOSED|   7556|\n",
    "# |SUSPECTED_FRAUD|   1558|\n",
    "# |        PENDING|   7610|\n",
    "# |       CANCELED|   1428|\n",
    "# +---------------+-------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### join/groupBy/sum/sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.join(order_itemsDF, ordersDF.order_id == order_itemsDF.order_item_order_id).\n",
    "        groupBy(\"order_date\", \"order_item_product_id\").\n",
    "        agg(sum(\"order_item_subtotal\").alias(\"Revenue\")).\n",
    "        show()\n",
    "ordersDF.sort([\"order_date\", \"order_customer_id\"], ascending=[1, 0]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import configparser as cp, sys\n",
    "\n",
    "props = cp.RawConfigParser()\n",
    "props.read(\"/Users/dbratu/Documents/big_data/application.properties\")\n",
    "env = sys.argv[1]\n",
    "inputBaseDir = props.get(env, \"input.base.dir\")\n",
    "outputBaseDir = props.get(env, \"output.base.dir\")\n",
    "\n",
    "sc = SparkContext(master=\"local\", appName=\"Daily_Revenue\")\n",
    "# spark = SparkSession(sc)\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"DailyRevenue\").\\\n",
    "    master(props.get(env, \"executionMode\")).\\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "orders = spark.\\\n",
    "    read.\\\n",
    "    format(\"csv\").\\\n",
    "    schema(\"\"\"order_id int,\n",
    "     order_date string,\n",
    "      order_customer_id int,\n",
    "       order_status string\"\"\").\\\n",
    "    load(inputBaseDir +\"/orders\")\n",
    "\n",
    "order_items = spark.\\\n",
    "    read.\\\n",
    "    format(\"csv\").\\\n",
    "    schema(\"\"\"order_item_id int,\n",
    "     order_item_order_id int,\n",
    "      order_item_product_id int,\n",
    "       order_item_quantity int,\n",
    "        order_item_subtotal float,\n",
    "         order_item_product_price float\"\"\").\\\n",
    "    load(inputBaseDir +\"/order_items\")\n",
    "\n",
    "daily_prod_rev = orders.\\\n",
    "    filter(\"order_status in ('CLOSED', 'COMPLETE')\").\\\n",
    "    join(order_items, orders.order_id == order_items.order_item_order_id).\\\n",
    "    groupBy(\"order_date\", \"order_item_order_id\").\\\n",
    "    agg(round(sum(\"order_item_subtotal\"), 2).alias(\"revenue\"))\n",
    "\n",
    "daily_prod_sort = daily_prod_rev.sort([daily_prod_rev.order_date, daily_prod_rev.revenue], ascending=[1, 0])\n",
    "\n",
    "daily_prod_sort.write.csv(outputBaseDir + \"/rev1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application.properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[dev]\n",
    "executionMode = local\n",
    "input.base.dir = /Users/dbratu/Documents/big_data/data-master/retail_db\n",
    "output.base.dir = /Users/dbratu/Documents/big_data/app_out\n",
    "\n",
    "[prod]\n",
    "executionMode = yarn-client\n",
    "input.base.dir = /Users/dbratu/Documents/big_data/data-master/retail_db\n",
    "output.base.dir = /Users/dbratu/Documents/big_data/app_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytics functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window.partitionBy(order_items.order_item_order_id)\n",
    "\n",
    "print(order_items.withColumn(\"revenue\", \n",
    "                             round(sum(\"order_item_subtotal\").\n",
    "                                              over(spec), 2)).\n",
    "      select(\"order_item_id\",\n",
    "             \"order_item_order_id\",\n",
    "             \"order_item_subtotal\",\n",
    "             \"revenue\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window.partitionBy('order_date')\n",
    "\n",
    "print(orders.select(\"order_id\", \n",
    "                    \"order_date\", \n",
    "                    \"order_customer_id\", \n",
    "                    \"order_status\", \n",
    "                    count('order_date').over(spec).alias(\"daily_count\")).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min/max/avg/pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employees = spark.\\\n",
    "    read.\\\n",
    "    format(\"csv\").\\\n",
    "    option('sep', '\\t').\\\n",
    "    schema(\"\"\"employee_id int,\n",
    "     f_name string,\n",
    "      l_name string,\n",
    "       email string,\n",
    "        phone_number string,\n",
    "         hire_date string,\n",
    "          department string,\n",
    "           salary float,\n",
    "            pct string,\n",
    "             department_id int,\n",
    "              manager_id int\"\"\").\\\n",
    "    load(\"/Users/dbratu/Documents/big_data/data-master/hr_db/employees\")\n",
    "\n",
    "spec = Window.partitionBy(\"department_id\")\n",
    "\n",
    "print(employees.\n",
    "      select(\"employee_id\", \"department_id\", \"salary\").\n",
    "      withColumn(\"salary_expense\", sum(\"salary\").over(spec)).\n",
    "      withColumn(\"least_salary\", min(\"salary\").over(spec)).\n",
    "      withColumn(\"maximum_salary\", max(\"salary\").over(spec)).\n",
    "      withColumn(\"average_salary\", avg(\"salary\").over(spec)).\n",
    "      withColumn(\"salary_pct\", round(employees.salary/sum(\"salary\").over(spec) * 100, 2)).\n",
    "      sort('department_id').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lead/lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window.partitionBy(\"department_id\").orderBy(employees.salary.desc())\n",
    "\n",
    "print(employees.\n",
    "      select(\"employee_id\", \"department_id\", \"salary\").\n",
    "      withColumn(\"next_employee\", lead(\"employee_id\").over(spec)).\n",
    "      withColumn(\"difference\", employees.salary - lead(\"salary\").over(spec)).\n",
    "      sort('department_id', employees.salary.desc()).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make lag work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window.partitionBy(\"department_id\").\\\n",
    "    orderBy(employees.salary.desc()).\\\n",
    "    rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "print(employees.\n",
    "      select(\"employee_id\", \"department_id\", \"salary\").\n",
    "      withColumn(\"last\", last(\"salary\", False).over(spec)).\n",
    "      sort('department_id', employees.salary.desc()).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = Window.partitionBy(\"department_id\").\\\n",
    "    orderBy(employees.salary.desc())\n",
    "\n",
    "print(employees.\n",
    "      select(\"employee_id\", \"department_id\", \"salary\").\n",
    "      withColumn(\"rank\", rank().over(spec)).\n",
    "      withColumn(\"dense_rank\", dense_rank().over(spec)).\n",
    "          withColumn(\"row_number\", row_number().over(spec)).\n",
    "      sort('department_id', employees.salary.desc()).show(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit \n",
    "    --master local\n",
    "    --conf spark.ui.port=4040 \n",
    "    --num-executors 2 \n",
    "    --executor-memory 512M\n",
    "    \\Users\\dbratu\\Desktop\\demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querries/Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a database\n",
    "`create database database_name;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a database\n",
    "`use database_name;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show tables/databases\n",
    "`show tables;`<br>\n",
    "`show databases;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table\n",
    "`create table orders (\n",
    "order_id int,\n",
    "order_date string,\n",
    "order_customer_id int,\n",
    "order_status string\n",
    ")row format delimited fields terminated by \",\" # the delimiter\n",
    "stored as textfile; # these are given in hive language manual during the certification`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into tables:\n",
    "- `load data local inpath \"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\" into table orders;` - local files\n",
    "- `load data inpath \"/Users/dbratu/Documents/big_data/hdfs_files/data-master/retail_db/orders\" into table orders;` - hdfs files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview data\n",
    "`select * from orders limit 10;`<br>\n",
    "`dfs -ls /Users/dbratu/Documents/big_data/hive_files/dbratu.db/orders;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store table as orc format\n",
    "`create table orders(\n",
    "order_id int,\n",
    "order_date string,\n",
    "order_customer_id int,\n",
    "order_status string\n",
    ") stored as orc;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into orc table\n",
    "`insert into table orders select * from dbratu.orders;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about table\n",
    "`describe orders;\n",
    "describe formatted orders;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running hive commands using pyspark\n",
    "`sqlContext.sql(\"show tables\").show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation\n",
    "- substr <br>\n",
    "`select substr(\"Dragos\", 2, 3);`-> `rag`\n",
    "- instr <br>\n",
    "`select instr(\"Dragos\", \"r\");` -> `2`\n",
    "- like <br>\n",
    "`select \"la la la tra\" like \"%tra\";` -> `true`\n",
    "- rlike (regular expressions) <br>\n",
    "- lcase=lower/ucase=upper <br>\n",
    "- initcap <br>\n",
    "- trim <br>\n",
    "`select trim(\" Dragos Bratu \");` -> `Dragos Bratu`\n",
    "- padding <br>\n",
    "`select lpad(\"Dra\", 4, \"#\");`-> `#Dra`\n",
    "- cast <br> \n",
    "`select cast(\"12\" as int);` -> `12`\n",
    "- split/index <br>\n",
    "`select index(split(\"Dragos Bratu Florian\", \" \"), 1);` -> `\"Bratu\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date manipulation\n",
    "- current date/timestamp <br>\n",
    "`select current_date;`\n",
    "- date_format <br>\n",
    "`select date_format(current_date, \"MM-dd/YYYY\");` -> `08-17/2020`\n",
    "    - \"d\" - day in the week\n",
    "    - \"D\" - day in the year\n",
    "- day <br>\n",
    "`select day(current_date);` -> `17`\n",
    "- to_date <br>\n",
    "`select to_date(current_timestamp);` -> `2020-08-17`\n",
    "- to_unix_timestamp <br>\n",
    "`select to_unix_timestamp(current_date);` -> `1597622400`\n",
    "- from_unixtime <br>\n",
    "`select from_unixtime(1597622400);` -> `2020-08-17 00:00:00`\n",
    "\n",
    "`select to_unix_timestamp(to_date(order_date)) from orders limit 3;`\n",
    "    - 1374710400\n",
    "    - 1374710400\n",
    "    - 1374710400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate functions\n",
    "- min\n",
    "- max\n",
    "- count\n",
    "- avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case vs nvl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select order_status,\n",
    "            case order_status\n",
    "            when \"CLOSED\" then \"No Action\"\n",
    "            when \"COMPLETE\" then \"No Action\"\n",
    "            end from orders limit 10;`\n",
    "\n",
    "`select order_status,\n",
    "            case\n",
    "                when order_status in (\"CLOSED\", \"COMPLETE\") then \"No Action\"\n",
    "                when order_status in (\"CANCELED\",\"ON_HOLD\", \"PAYMENT_REVIEW\", \"PENDING\", \"PENDING_PAYMENT\",\"PROCESSING\",\"SUSPECTED_FRAUD\") then \"Action\"\n",
    "                else \"Risky\"\n",
    "                end from orders limit 10;`\n",
    "- CLOSED\tNo Action\n",
    "- PENDING_PAYMENT\tAction\n",
    "- COMPLETE\tNo Action\n",
    "- CLOSED\tNo Action\n",
    "- COMPLETE\tNo Action\n",
    "- COMPLETE\tNo Action\n",
    "- COMPLETE\tNo Action\n",
    "- PROCESSING\tAction\n",
    "- PENDING_PAYMENT\tAction\n",
    "- PENDING_PAYMENT\tAction\n",
    "\n",
    "`select case when order_status is null then \"Data missing\" else order_status end from orders limit 10;`\n",
    "\n",
    "or\n",
    "\n",
    "`select nvl(order_status, \"Data missing\") from orders limit 10;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row level transformation\n",
    "`select cast(concat(substr(order_date, 1, 4), substr(order_date, 6, 2)) as int) from orders limit 10;`\n",
    "\n",
    "or\n",
    "\n",
    "`select cast(date_format(order_date, \"YYYYMM\") as int) from orders limit 10;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join\n",
    "- wrong way <br>\n",
    "`select o.*, c.* from orders o, customers c\n",
    "where o.order_customer_id = c.customer_id\n",
    "limit 10;`\n",
    "- right way <br>\n",
    "`select o.*, c.* from orders o join customers c\n",
    "on o.order_customer_id = c.customer_id\n",
    "limit 10;`\n",
    "\n",
    "#### Left/Right outer join\n",
    "`select o.*, c.* from customers c left outer join orders o\n",
    "on o.order_customer_id = c.customer_id\n",
    "limit 10;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by and aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only the column present in group by can be present besides the aggregate function\n",
    "\n",
    "`select order_status, count(1) from orders\n",
    "group by order_status;`\n",
    "\n",
    "`CANCELED\t2856\n",
    "CLOSED\t15112\n",
    "COMPLETE\t45798`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`select o.order_id, sum(oi.order_item_subtotal) order_revenue\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")\n",
    "group by o.order_id\n",
    "having sum(oi.order_item_subtotal) >= 10000;`\n",
    "\n",
    "`68703\t13799.639953613281\n",
    "68724\t11439.560012817383\n",
    "68778\t10519.599975585938\n",
    "68806\t10519.680053710938\n",
    "68821\t10519.680053710938\n",
    "68858\t11359.640014648438`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting\n",
    "\n",
    "You can also use alis in order by clause (only here)\n",
    "\n",
    "`select o.order_id, round(sum(oi.order_item_subtotal), 2) order_revenue\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")\n",
    "group by o.order_id\n",
    "having sum(oi.order_item_subtotal) >= 10000\n",
    "order by sum(oi.order_item_subtotal);`\n",
    "\n",
    "`68703\t13799.64\n",
    "68724\t11439.56\n",
    "68858\t11359.64\n",
    "68821\t10519.68\n",
    "68806\t10519.68\n",
    "68778\t10519.6`\n",
    "\n",
    "`select o.order_id, o.order_date, o.order_status, round(sum(oi.order_item_subtotal), 2) order_revenue\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")\n",
    "group by o.order_id, o.order_date, o.order_status\n",
    "having sum(oi.order_item_subtotal) >= 3000\n",
    "distribute by o.order_date sort by o.order_date, order_revenue desc;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Union all__\n",
    "\n",
    "`select 1, \"Dragos\"\n",
    "union all\n",
    "select 2, \"Bratu\"\n",
    "union all\n",
    "select 1, \"Dragos\"\n",
    "union all\n",
    "select 2, \"Florian\";`\n",
    "- 1\tDragos\n",
    "- 2\tBratu\n",
    "- 1\tDragos\n",
    "- 2\tFlorian\n",
    "\n",
    "__Union (remove duplicates)__\n",
    "\n",
    "`select 1, \"Dragos\"\n",
    "union\n",
    "select 2, \"Bratu\"\n",
    "union\n",
    "select 1, \"Dragos\"\n",
    "union\n",
    "select 2, \"Florian\";`\n",
    "\n",
    "- 1\tDragos\n",
    "- 2\tBratu\n",
    "- 2\tFlorian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing and Analytics functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations\n",
    "- SUM, AVG\n",
    "\n",
    "`select * from (\n",
    "select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal,\n",
    "round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2) order_revenue,\n",
    "order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2) pct_revenue,\n",
    "round(avg(oi.order_item_subtotal) over (partition by o.order_id), 2) avg_revenue\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")) q\n",
    "where order_revenue >= 3000\n",
    "order by order_date, order_revenue desc;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking\n",
    "\n",
    "`select * from (\n",
    "select o.order_id, o.order_date, o.order_status, oi.order_item_subtotal,\n",
    "round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2) order_revenue,\n",
    "order_item_subtotal/round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2) pct_revenue,\n",
    "round(avg(oi.order_item_subtotal) over (partition by o.order_id), 2) avg_revenue,\n",
    "rank() over (partition by o.order_id order by oi.order_item_subtotal desc) rnk_revenue,\n",
    "dense_rank() over (partition by o.order_id order by oi.order_item_subtotal desc) dense_rnk_revenue,\n",
    "percent_rank() over (partition by o.order_id order by oi.order_item_subtotal desc) pct_rnk_revenue,\n",
    "row_number() over (partition by o.order_id order by oi.order_item_subtotal desc) rn_ordered_revenue,\n",
    "row_number() over (partition by o.order_id) rn_revenue\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")) q\n",
    "where order_revenue >= 1000\n",
    "order by order_date, order_revenue desc, rnk_revenue;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing functions\n",
    "LEAD, LAG FIRST_VALUE, LAST_VALUE\n",
    "\n",
    "`select * from (\n",
    "select o.order_id, o.order_date, oi.order_item_subtotal,\n",
    "round(sum(oi.order_item_subtotal) over (partition by o.order_id), 2) order_revenue,\n",
    "lead(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) lead_order_item_subtotal,\n",
    "lag(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) lag_order_item_subtotal,\n",
    "first_value(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) fv_order_item_subtotal,\n",
    "last_value(oi.order_item_subtotal) over (partition by o.order_id order by oi.order_item_subtotal desc) lv_order_item_subtotal\n",
    "from orders o join order_items oi\n",
    "on o.order_id = oi.order_item_order_id\n",
    "where o.order_status in (\"COMPLETE\", \"CLOSED\")) q\n",
    "where order_revenue >= 1000\n",
    "order by order_date, order_revenue desc;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temp tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For pyspark shell\n",
    "\n",
    "`ordersRDD = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\")`\n",
    "\n",
    "`ordersDF = ordersRDD.map(lambda o: Row(order_id=int(o.split(\",\")[0]), order_date=o.split(\",\")[1], order_customer_id=int(o.split(\",\")[2]), order_status=o.split(\",\")[3])).toDF()\n",
    "ordersDF.show()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For pycharm\n",
    "\n",
    "`from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import Row, SparkSession`\n",
    "\n",
    "`sc = SparkContext(master=\"local\", appName=\"Spark Demo\")\n",
    "spark = SparkSession(sc)`\n",
    "\n",
    "`ordersRDD = sc.textFile(\"/Users/dbratu/Documents/big_data/data-master/retail_db/orders\")\n",
    "ordersDF = ordersRDD.map(lambda o: Row(order_id=int(o.split(\",\")[0]), order_date=o.split(\",\")[1], order_customer_id=int(o.split(\",\")[2]), order_status=o.split(\",\")[3])).toDF()\n",
    "print(ordersDF.show())`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a temporary table\n",
    "\n",
    "`ordersDF.registerTempTable(\"ordersDF_table\")\n",
    "sqlContext.sql(\"select * from ordersDF_table limit 10;\").show()`\n",
    "\n",
    "`+--------+--------------------+-----------------+---------------+\n",
    "|order_id|          order_date|order_customer_id|   order_status|\n",
    "+--------+--------------------+-----------------+---------------+\n",
    "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
    "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
    "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
    "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
    "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
    "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
    "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
    "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
    "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
    "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
    "+--------+--------------------+-----------------+---------------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sqlContext.sql(\"select order_status, count(1) as count from ordersDF_table group by order_status;\").show()`\n",
    "\n",
    "`+---------------+-----+\n",
    "|   order_status|count|\n",
    "+---------------+-----+\n",
    "|PENDING_PAYMENT|15030|\n",
    "|       COMPLETE|22899|\n",
    "|        ON_HOLD| 3798|\n",
    "| PAYMENT_REVIEW|  729|\n",
    "|     PROCESSING| 8275|\n",
    "|         CLOSED| 7556|\n",
    "|SUSPECTED_FRAUD| 1558|\n",
    "|        PENDING| 7610|\n",
    "|       CANCELED| 1428|\n",
    "+---------------+-----+`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table starting from local file sistem data\n",
    "\n",
    "`from pyspark.sql import Row, SparkSession\n",
    "productsRAW = open(\"/Users/dbratu/Documents/big_data/data-master/retail_db/products/part-00000\").read().splitlines()\n",
    "productsRDD = sc.parallelize(productsRAW)\n",
    "productsDF = productsRDD.map(lambda o: Row(product_id=int(o.split(\",\")[0]), product_name=o.split(\",\")[2])).toDF()\n",
    "productsDF.registerTempTable(\"products\")\n",
    "sqlContext.sql(\"show tables;\").show()\n",
    "`\n",
    "\n",
    "`+--------+-----------+-----------+\n",
    "|database|  tableName|isTemporary|\n",
    "+--------+-----------+-----------+\n",
    "|  dbratu|  customers|      false|\n",
    "|  dbratu|order_items|      false|\n",
    "|  dbratu|     orders|      false|\n",
    "|        |   products|       true|\n",
    "+--------+-----------+-----------+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join operation\n",
    "\n",
    "`sqlContext.sql(\"\\\n",
    "SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue \\\n",
    "FROM orders o \\\n",
    "JOIN order_items oi \\\n",
    "ON o.order_id = oi.order_item_order_id \\\n",
    "JOIN products p \\\n",
    "ON p.product_id = oi.order_item_order_id \\\n",
    "WHERE o.order_status IN ('CLOSED', 'COMPLETE') \\\n",
    "GROUP BY o.order_date, p.product_name \\\n",
    "ORDER BY o.order_date, daily_revenue DESC;\").show()\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write dataframes to hive tables\n",
    "\n",
    "`sqlContext.sql(\"\\\n",
    "CREATE TABLE daily_revenue \\\n",
    "(order_date string, \\\n",
    "product_name string, \\\n",
    "daily_revenue float\\\n",
    ") row format delimited fields terminated by ',' \\\n",
    "stored as textfile;\")`\n",
    "\n",
    "`daily_revenueDF = sqlContext.sql(\"\\\n",
    "SELECT o.order_date, p.product_name, sum(oi.order_item_subtotal) daily_revenue \\\n",
    "FROM orders o \\\n",
    "JOIN order_items oi \\\n",
    "ON o.order_id = oi.order_item_order_id \\\n",
    "JOIN products p \\\n",
    "ON p.product_id = oi.order_item_order_id \\\n",
    "WHERE o.order_status IN ('CLOSED', 'COMPLETE') \\\n",
    "GROUP BY o.order_date, p.product_name \\\n",
    "ORDER BY o.order_date, daily_revenue DESC;\")`\n",
    "\n",
    "`daily_revenueDF.write.insertInto(\"dbratu.daily_revenue\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems\n",
    "1.  1\n",
    "    b. 2 \n",
    "2. 10\n",
    "3. 12\n",
    "4. 13\n",
    "5. 15\n",
    "6. 19\n",
    "7. 23\n",
    "    b. 24 x\n",
    "    b. 27 x\n",
    "8. 28\n",
    "9. 32\n",
    "    b. 34 x\n",
    "10. 41\n",
    "11. 46\n",
    "12. 48\n",
    "13. 50\n",
    "14. 52    x\n",
    "15. 53\n",
    "    b. 57 x\n",
    "16. 58\n",
    "17. 60\n",
    "18. 63\n",
    "19. 65\n",
    "20. 79\n",
    "21. 80\n",
    "22. 81\n",
    "23. 83\n",
    "24. 89\n",
    "    b. 91 x\n",
    "25. 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 1\n",
    "You have been given MySQL DB with following details.\n",
    "user=retail_dba\n",
    "password=cloudera\n",
    "database=retail_db\n",
    "table=retail_db.categories\n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following activities.\n",
    "1. Connect MySQL DB and check the content of the tables.\n",
    "2. Copy \"retaildb.categories\" table to hdfs, without specifying directory name.\n",
    "3. Copy \"retaildb.categories\" table to hdfs, in a directory name \"categories_target\".\n",
    "4. Copy \"retaildb.categories\" table to hdfs, in a warehouse directory name\n",
    "\"categories_warehouse\".\n",
    "##############################################################################################\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(appName=\"app\", master=\"local\")\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"app\").\\\n",
    "    master('local').\\\n",
    "    getOrCreate()\n",
    "\n",
    "departments = spark.\\\n",
    "    read.\\\n",
    "    format(\"jdbc\").\\\n",
    "    options(url=\"jdbc:mysql://quickstart:3306/retail_db\",\n",
    "            driver = \"com.mysql.jdbc.Driver\",\n",
    "            dbtable = \"categories\",\n",
    "            user=\"retail_dba\",\n",
    "            password=\"cloudera\").\\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 2\n",
    "There is a parent organization called \"ABC Group Inc\", which has two child companies named \n",
    "Tech Inc and MPTech.\n",
    "Both companies employee information is given in two separate text file as below. \n",
    "Please do the following activity for employee details.\n",
    "\n",
    "Tech Inc.txt -\n",
    "1,Alok,Hyderabad\n",
    "2,Krish,Hongkong\n",
    "3,Jyoti,Mumbai\n",
    "4,Atul,Banglore\n",
    "5,Ishan,Gurgaon\n",
    "\n",
    "MPTech.txt -\n",
    "6,John,Newyork\n",
    "7,alp2004,California\n",
    "8,tellme,Mumbai\n",
    "9,Gagan21,Pune\n",
    "10,Mukesh,Chennai\n",
    "\n",
    "1. Which command will you use to check all the available command line options on HDFS and \n",
    "How will you get the Help for individual command.\n",
    "2. Create a new Empty Directory named Employee using Command line. And also create an empty \n",
    "file named in it Techinc.txt\n",
    "3. Load both companies Employee data in Employee directory \n",
    "(How to override existing file in HDFS).\n",
    "4. Merge both the Employees data in a Single tile called MergedEmployee.txt, \n",
    "merged tiles should have new line character at the end of each file content.\n",
    "5. Upload merged file on HDFS and change the file permission on HDFS merged file, \n",
    "so that owner and group member can read and write, other user can read the file.\n",
    "6. Write a command to export the individual file as well as entire directory from HDFS to \n",
    "local file System.\n",
    "##############################################################################################\\\n",
    "1. hdfs dfs\n",
    "2. hdfs dfs -mkdir Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 10\n",
    "You have been given following mysql database details as well as other info. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following.\n",
    "1. Create a database named hadoopexam and then create a table named departments in it, \n",
    "with following fields. department_id int, department_name string e.g. location should be \n",
    "hdfs://quickstart.cloudera:8020/user/hive/warehouse/hadoopexam.db/departments\n",
    "2. Please import data in existing table created above from retaidb.departments into hive \n",
    "table hadoopexam.departments.\n",
    "3. Please import data in a non-existing table, means while importing create hive table \n",
    "named hadoopexam.departments_new\n",
    "##############################################################################################\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 12\n",
    "You have been given following mysql database details as well as other info. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following.\n",
    "1. Create a table in retailedb with following definition.\n",
    "CREATE table departments_new (department_id int(11), department_name varchar(45), \n",
    "                              created_date T1MESTAMP DEFAULT NOW());\n",
    "2. Now isert records from departments table to departments_new\n",
    "3. Now import data from departments_new table to hdfs.\n",
    "4. Insert following 5 records in departmentsnew table. \n",
    "Insert into departments_new values(110, \"Civil\" , null); \n",
    "Insert into departments_new values(111, \"Mechanical\" , null);\n",
    "Insert into departments_new values(112, \"Automobile\" , null); \n",
    "Insert into departments_new values(113, \"Pharma\" , null);\n",
    "Insert into departments_new values(114, \"Social Engineering\" , null);\n",
    "5. Now do the incremental import based on created_date column.\n",
    "##############################################################################################\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 13\n",
    "You have been given following mysql database details as well as other info. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following.\n",
    "1. Create a table in retailedb with following definition.\n",
    "CREATE table departments_export (department_id int(11), department_name varchar(45), \n",
    "                                 created_date T1MESTAMP DEFAULT NOWQ);\n",
    "2. Now import the data from following directory into departments_export table,\n",
    "/user/cloudera/departments new\n",
    "##############################################################################################\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 15\n",
    "You have been given following mysql database details as well as other info. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following activities.\n",
    "1. In mysql departments table please insert following record. \n",
    "Insert into departments values(9999, '\"Data Science\"1);\n",
    "2. Now there is a downstream system which will process dumps of this file. \n",
    "However, system is designed the way that it can process only files if fields\n",
    "are enlcosed in(') single quote and separate of the field should be (-} and line needs \n",
    "to be terminated by : (colon).\n",
    "3. If data itself contains the \" (double quote } than it should be escaped by \\.\n",
    "4. Please import the departments table in a directory called departments_enclosedby and \n",
    "file should be able to process by downstream system.\n",
    "##############################################################################################\n",
    "1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 19\n",
    "You have been given following mysql database details as well as other info. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "            \n",
    "Now accomplish following activities.\n",
    "1. Import departments table from mysql to hdfs as textfile in departments_text directory.\n",
    "2. Import departments table from mysql to hdfs as sequncefile in departments_sequence directory.\n",
    "3. Import departments table from mysql to hdfs as avro file in departments avro directory.\n",
    "4. Import departments table from mysql to hdfs as parquet file in departments_parquet directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 23\n",
    "You have been given log generating service as below.\n",
    "Start_logs (It will generate continuous logs)\n",
    "Tail_logs (You can check , what logs are being generated)\n",
    "Stop_logs (It will stop the log service)\n",
    "Path where logs are generated using above service : /opt/gen_logs/logs/access.log\n",
    "Now write a flume configuration file named flume3.conf , using that configuration file \n",
    "dumps logs in HDFS file system in a directory called flumeflume3/%Y/%m/%d/%H/%M\n",
    "Means every minute new directory should be created). Please us the interceptors to \n",
    "provide timestamp information, if message header does not have header info.\n",
    "And also note that you have to preserve existing timestamp, if message contains it. \n",
    "Flume channel should have following property as well. After every 100 message it should \n",
    "be committed, use non-durable/faster channel and it should be able to hold maximum 1000 \n",
    "events.\n",
    "##############################################################################################\n",
    "FLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 24\n",
    "You have been given below comma separated employee information.\n",
    "Data Set:\n",
    "name,salary,sex,age\n",
    "alok,100000,male,29\n",
    "jatin,105000,male,32\n",
    "yogesh,134000,male,39\n",
    "ragini,112000,female,35\n",
    "jyotsana,129000,female,39\n",
    "valmiki,123000,male,29\n",
    "\n",
    "Use the netcat service on port 44444, and nc above data line by line. \n",
    "Please do the following activities.\n",
    "1. Create a flume conf file using fastest channel, which write data in hive warehouse \n",
    "directory, in a table called flumemaleemployee (Create hive table as well tor given data).\n",
    "2. While importing, make sure only male employee data is stored.\n",
    "##############################################################################################\n",
    "# Step 1 : Create hive table for flumeemployee\n",
    "CREATE TABLE flumemaleemployee\n",
    "(\n",
    "name string,\n",
    "salary int,\n",
    "sex string,\n",
    "age int\n",
    ")\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "# Step 2 : Create flume configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 27\n",
    "You need to implement near real time solutions for collecting information when submitted \n",
    "in file with below information.\n",
    "\n",
    "Data -\n",
    "echo \"IBM,100,20160104\" >> /tmp/spooldir/bb/.bb.txt\n",
    "echo \"IBM,103,20160105\" >> /tmp/spooldir/bb/.bb.txt\n",
    "mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt\n",
    "\n",
    "After few mins -\n",
    "echo \"IBM,100.2,20160104\" >> /tmp/spooldir/dr/.dr.txt\n",
    "echo \"IBM,103.1,20160105\" >> /tmp/spooldir/dr/.dr.txt\n",
    "mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt\n",
    "\n",
    "You have been given below directory location (if not available than create it) \n",
    "/tmp/spooldir .\n",
    "You have a finacial subscription for getting stock prices from BloomBerg as well \n",
    "as Reuters and using ftp you download every hour new files from their respective ftp site \n",
    "in directories /tmp/spooldir/bb and /tmp/spooldir/dr respectively.\n",
    "As soon as file committed in this directory that needs to be available in hdfs in\n",
    "/tmp/flume/finance location in a single directory.\n",
    "Write a flume configuration file named flume7.conf and use it to load data in hdfs with \n",
    "following additional properties:\n",
    "    1. Spool /tmp/spooldir/bb and /tmp/spooldir/dr\n",
    "    2. File prefix in hdfs sholuld be events\n",
    "    3. File suffix should be .log\n",
    "    4. If file is not commited and in use than it should have _ as prefix.\n",
    "    5. Data should be written as text to hdfs\n",
    "##############################################################################################\n",
    "# FLUME APP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 28\n",
    "You need to implement near real time solutions for collecting information when \n",
    "submitted in file with below\n",
    "\n",
    "Data -\n",
    "echo \"IBM,100,20160104\" >> /tmp/spooldir2/.bb.txt\n",
    "echo \"IBM,103,20160105\" >> /tmp/spooldir2/.bb.txt\n",
    "mv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt\n",
    "\n",
    "After few mins -\n",
    "echo \"IBM,100.2,20160104\" >> /tmp/spooldir2/.dr.txt\n",
    "echo \"IBM,103.1,20160105\" >> /tmp/spooldir2/.dr.txt\n",
    "mv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt\n",
    "\n",
    "You have been given below directory location (if not available than create it) \n",
    "/tmp/spooldir2\n",
    "As soon as file committed in this directory that needs to be available in hdfs in\n",
    "/tmp/flume/primary as well as /tmp/flume/secondary location.\n",
    "However, note that/tmp/flume/secondary is optional, if transaction failed which writes \n",
    "in this directory need not to be rollback.\n",
    "Write a flume configuration file named flumeS.conf and use it to load data in hdfs with \n",
    "following additional properties .\n",
    "1. Spool /tmp/spooldir2 directory\n",
    "2. File prefix in hdfs sholuld be events\n",
    "3. File suffix should be .log\n",
    "4. If file is not committed and in use than it should have _ as prefix.\n",
    "5. Data should be written as text to hdfs\n",
    "##############################################################################################\n",
    "FLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 32\n",
    "You have given three files as below.\n",
    "spark3/sparkdir1/file1.txt\n",
    "spark3/sparkd ir2ffile2.txt\n",
    "spark3/sparkd ir3Zfile3.txt\n",
    "Each file contain some text.\n",
    "\n",
    "spark3/sparkdir1/file1.txt\n",
    "Apache Hadoop is an open-source software framework written in Java for distributed storage \n",
    "and distributed processing of very large data sets on computer clusters built from commodity \n",
    "hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware \n",
    "failures are common and should be automatically handled by the framework \n",
    "\n",
    "spark3/sparkdir2/file2.txt\n",
    "The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File\n",
    "System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks \n",
    "and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged \n",
    "code for nodes to process in parallel based on the data that needs to be processed. \n",
    "\n",
    "spark3/sparkdir3/file3.txt \n",
    "his approach takes advantage of data locality nodes manipulating the data they have access \n",
    "to to allow the dataset to be processed faster and more efficiently than it would be in a \n",
    "more conventional supercomputer architecture that relies on a parallel file system where \n",
    "computation and data are distributed via high-speed networking\n",
    "\n",
    "\n",
    "Now write a Spark code in scala which will load all these three files from hdfs and do \n",
    "the word count by filtering following words. And result should be sorted by word count \n",
    "in reverse order.\n",
    "Filter words (\"a\",\"the\",\"an\", \"as\", \"a\",\"with\",\"this\",\"these\",\"is\",\"are\",\"in\", \"for\",\n",
    "\"to\",\"and\",\"The\",\"of\")\n",
    "Also please make sure you load all three files as a Single RDD \n",
    "(All three files must be loaded using single API call).\n",
    "You have also been given following codec\n",
    "import org.apache.hadoop.io.compress.GzipCodec\n",
    "Please use above codec to compress file, while saving in hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 34\n",
    "You have given a file:\n",
    "\n",
    "user.csv\n",
    "    id,topic,hits\n",
    "    Rahul,scala,120 -\n",
    "    Nikita,spark,80 -\n",
    "    Mithun,spark,1 -\n",
    "    myself,cca175,180\n",
    "\n",
    "Now write a Spark code in scala which will remove the header part and create RDD of values \n",
    "as below, for all rows. And also if id is myself\" than filter out row.\n",
    "Map(id -> om, topic -> scala, hits -> 120)\n",
    "##############################################################################################\n",
    "a = spark.read.csv(path=\"/Users/dbratu/Documents/big_data/1.csv\")\n",
    "header = a.first()\n",
    "b = a.filter((a._c0 != header._c0) & (a._c0 != 'myself'))\n",
    "b.write.csv(\"/Users/dbratu/Documents/big_data/2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 41\n",
    "You have been given below code snippet. \n",
    "val aul = sc.parallelize(List ((\"a\" , Array(1,2)), (\"b\" , Array(1,2)))) \n",
    "val au2 = sc.parallelize(List ((\"a\" , Array(3)), (\"b\" , Array(2))))\n",
    "\n",
    "Apply the Spark method, which will generate below output.\n",
    "Array[(String, Array[lnt])] = Array((a,Array(1, 2)), \n",
    "                                    (b,Array(1, 2)), \n",
    "                                    (a(Array(3)), (b,Array(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 46\n",
    "You have been given belwo list in scala (name,sex,cost) for each work done.\n",
    "List( (\"Deeapak\" , \"male\", 4000), (\"Deepak\" , \"male\", 2000), (\"Deepika\" , \"female\",\n",
    "2000),(\"Deepak\" , \"female\", 2000), (\"Deepak\" , \"male\", 1000) , (\"Neeta\" , \"female\", 2000))\n",
    "\n",
    "Now write a Spark program to load this list as an RDD and do the sum of cost for combination\n",
    "of name and sex (as key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 48\n",
    "You have been given below Python code snippet, with intermediate output.\n",
    "We want to take a list of records about people and then we want to sum up their ages \n",
    "and count them.\n",
    "So for this example the type in the RDD will be a Dictionary in the format of \n",
    "{name: NAME, age:AGE, gender:GENDER}.\n",
    "The result type will be a tuple that looks like so (Sum of Ages, Count) \n",
    "people = [] people.append({'name':'Amit', 'age':45,'gender':'M'}) \n",
    "people.append({'name':'Ganga', 'age':43,'gender':'F'}) \n",
    "people.append({'name':'John', 'age':28,'gender':'M'}) \n",
    "people.append({'name':'Lolita', 'age':33,'gender':'F'}) \n",
    "people.append({'name':'Dont Know', 'age':18,'gender':'T'}) \n",
    "peopleRdd=sc.parallelize(people) //Create an RDD \n",
    "peopleRdd.aggregate((0,0), seqOp, combOp) //Output of above line : 167, 5)\n",
    "Now define two operation seqOp and combOp , such that\n",
    "seqOp : Sum the age of all people as well count them, in each partition. combOp :\n",
    "Combine results from all partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 50\n",
    "You have been given below code snippet (calculating an average score}, \n",
    "with intermediate output. \n",
    "type ScoreCollector = (Int, Double) \n",
    "type PersonScores = (String, (Int, Double)) \n",
    "val initialScores = Array((\"Fred\", 88.0), (\"Fred\", 95.0), (\"Fred\", 91.0), (\"Wilma\", 93.0),\n",
    "(\"Wilma\", 95.0), (\"Wilma\", 98.0))\n",
    "val wilmaAndFredScores = sc.parallelize(initialScores).cache() \n",
    "val scores = wilmaAndFredScores.combineByKey(createScoreCombiner, scoreCombiner, scoreMerger) \n",
    "val averagingFunction = (personScore: PersonScores) => { val (name, (numberScores, totalScore)) = personScore (name, totalScore / numberScores) \n",
    "val averageScores = scores.collectAsMap(}.map(averagingFunction)\n",
    "Expected output: averageScores: scala.collection.Map[String,Double] = Map(Fred ->\n",
    "91.33333333333333, Wilma -> 95.33333333333333)\n",
    "Define all three required function , which are input for combineByKey method, e.g.\n",
    "(createScoreCombiner, scoreCombiner, scoreMerger). And help us producing required results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Scenario 52\n",
    "\"\"\"You have been given below code snippet:\n",
    "b = sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1])\n",
    "Write a correct code snippet which will produce below output:\n",
    "(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> 5, 2 -> 3,4 -> 2, 7 ->1)\"\"\"\n",
    "\n",
    "x = sc.parallelize([1, 2, 3, 4, 5, 5 ,6 ,8, 8, 8]).countByValue()\n",
    "for item in x.items():\n",
    "    print(item[0], '->', item[1])\n",
    "# 1 -> 6\n",
    "# 2 -> 3\n",
    "# .\n",
    "# .\n",
    "# 8 -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 53\n",
    "You have been given below code snippet. \n",
    "val a = sc.parallelize(1 to 10, 3) \n",
    "operation1 b.collect\n",
    "\n",
    "Output 1 -\n",
    "Array[lnt] = Array(2, 4, 6, 8,10)\n",
    "operation2\n",
    "\n",
    "Output 2 -\n",
    "Array[lnt] = Array(1,2, 3)\n",
    "Write a correct code snippet for operation1 and operation2 which will produce desired output, \n",
    "shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 57\n",
    "You have been given below code snippet:\n",
    "a = sc.parallelize(range(1, 10))\n",
    "Write a correct code snippet which will produce:\n",
    "((even,[2, 4, 6, 8]), (odd,[1, 3, 5, 7,9]))\n",
    "##############################################################################################\n",
    "a = sc.parallelize(range(1, 10))\n",
    "b = a.groupBy(lambda x: 'even' if x%2 == 0 else 'odd')\n",
    "for item in b.collect(): \n",
    "    print(item[0], ': ', list(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 58\n",
    "You have been given below code snippet. \n",
    "val a = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"spider\", \"eagle\"), 2) \n",
    "val b = a.keyBy(_.length) operation1\n",
    "\n",
    "Write a correct code snippet for operationl which will produce desired output, shown below.\n",
    "Array[(lnt, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)),\n",
    "(3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 60\n",
    "You have been given below code snippet. \n",
    "val a = sc.parallelize(List(\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"}, 3} \n",
    "val b = a.keyBy(_.length) \n",
    "val c = sc.parallelize(List(\"dog\",\"cat\",\"gnu\",\"salmon\",\"rabbit\",\"turkey\",\"woif\",\"bear\",\"bee\"), 3) \n",
    "val d = c.keyBy(_.length) operation1\n",
    "                            \n",
    "Write a correct code snippet for operationl which will produce desired output, shown below.\n",
    "Array[(lnt, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)),\n",
    "(6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)),\n",
    "(6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)),\n",
    "(3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 63 : You have been given below code snippet. \n",
    "    val a = sc.parallelize(List(\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"), 2) \n",
    "    val b = a.map(x => (x.length, x)) operation1\n",
    "Write a correct code snippet for operationl which will produce desired output, shown below.\n",
    "Array[(lnt, String}] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 65\n",
    "You have been given below code snippet. \n",
    "val a = sc.parallelize(List(\"dog\", \"cat\", \"owl\", \"gnu\", \"ant\"), 2) \n",
    "val b = sc.parallelize(1 to a.count.tolnt, 2) \n",
    "val c = a.zip(b) operation1\n",
    "Write a correct code snippet for operationl which will produce desired output, shown below.\n",
    "Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2>, (ant,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 79\n",
    "You have been given MySQL DB with following details. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "table=retail_db.orders \n",
    "table=retail_db.order_items \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "            \n",
    "Columns of products table : \n",
    "(product_id | product categoryid | product_name | product_description | product_prtce | product_image )\n",
    "\n",
    "Please accomplish following activities.\n",
    "1. Copy \"retaildb.products\" table to hdfs in a directory p93_products\n",
    "2. Filter out all the empty prices\n",
    "3. Sort all the products based on price in both ascending as well as descending order.\n",
    "4. Sort all the products based on price as well as product_id in descending order.\n",
    "5. Use the below functions to do data ordering or ranking and fetch top 10 elements \n",
    "top() takeOrdered() sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 80\n",
    "You have been given MySQL DB with following details. \n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "table=retail_db.products \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "Columns of products table : \n",
    "(product_id | product_category_id | product_name | product_description | product_price | product_image )\n",
    "\n",
    "Please accomplish following activities.\n",
    "1. Copy \"retaildb.products\" table to hdfs in a directory p93_products\n",
    "2. Now sort the products data sorted by product price per category, \n",
    "use productcategoryid colunm to group by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem Scenario 81:\n",
    "\"\"\"You have been given MySQL DB with following details:\n",
    "You have been given following product.csv file:\n",
    "productID,productCode,name,quantity,price\n",
    "1001,PEN,Pen Red,5000,1.23\n",
    "1002,PEN,Pen Blue,8000,1.25\n",
    "1003,PEN,Pen Black,2000,1.25\n",
    "1004,PEC,Pencil 2B,10000,0.48\n",
    "1005,PEC,Pencil 2H,8000,0.49\n",
    "1006,PEC,Pencil HB,0,9999.99\n",
    "\n",
    "Accomplish following activities.\n",
    "1. Create a Hive ORC table using SparkSql\n",
    "2. Load this data in Hive table.\n",
    "3. Create a Hive parquet table using SparkSQL and load data in it.\"\"\"\n",
    "\n",
    "# Create df\n",
    "b = spark.\n",
    "        read.\\\n",
    "        format('csv').\\\n",
    "        schema(\"\"\"product_id int, \n",
    "               product_code string, \n",
    "               name string, \n",
    "               quantity int, \n",
    "               price float\"\"\").\\\n",
    "load(\"/Users/dbratu/Documents/big_data/1.csv\")\n",
    "b.show()\n",
    "b.createTempView('tbl1')\n",
    "\n",
    "# or create directly as hive tbl\n",
    "create table tbl1(product_id int, \n",
    "                  product_code string, \n",
    "                  name string, \n",
    "                  quantity int, \n",
    "                  price float)\n",
    "row format delimited fields terminated by ',' stored as textfile;\n",
    "\n",
    "load data local inpath '/Users/dbratu/Documents/big_data/1.csv' into table tbl1;\n",
    "\n",
    "# Create orc and parquet tables and load data into it\n",
    "spark.sql(\"\"\"create table tbl2(product_id int, \n",
    "                  product_code string, \n",
    "                  name string, \n",
    "                  quantity int, \n",
    "                  price float)\n",
    "stored as orc;\"\"\")\n",
    "spark.sql(\"\"\"insert into tbl3 select * from tbl1;\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"create table tbl_parquet(product_id int, \n",
    "                  product_code string, \n",
    "                  name string, \n",
    "                  quantity int, \n",
    "                  price float)\n",
    "stored as parquet;\"\"\")\n",
    "spark.sql(\"\"\"insert into tbl_parquet select * from tbl1;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 83\n",
    "In Continuation of previous question, please accomplish following activities.\n",
    "1. Select all the records with quantity >= 5000 and name starts with 'Pen'\n",
    "2. Select all the records with quantity >= 5000, price is less than 1.24 and name starts with\n",
    "'Pen'\n",
    "3. Select all the records witch does not have quantity >= 5000 and name does not starts with 'Pen'\n",
    "4. Select all the products which name is 'Pen Red', 'Pen Black'\n",
    "5. Select all the products which has price BETWEEN 1.0 AND 2.0 AND quantity\n",
    "BETWEEN 1000 AND 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 89\n",
    "You have been given below patient data in csv format, \n",
    "patientID,name,dateOfBirth,lastVisitDate\n",
    "1001,Ah Teck,1991-12-31,2012-01-20\n",
    "1002,Kumar,2011-10-29,2012-09-20\n",
    "1003,Ali,2011-01-30,2012-10-21\n",
    "\n",
    "Accomplish following activities.\n",
    "1. Find all the patients whose lastVisitDate between current time and '2012-09-15'\n",
    "2. Find all the patients who born in 2011\n",
    "3. Find all the patients age\n",
    "4. List patients whose last visited more than 60 days ago\n",
    "5. Select patients 18 years old or younger\n",
    "##############################################################################################\n",
    "some = spark.\\\n",
    "            read.\\\n",
    "            format('csv').\\\n",
    "            option('csv', ',').\\\n",
    "            load('/Users/dbratu/Documents/big_data/81.csv', header=True)\n",
    "some.show()\n",
    "# +---------+-------+-----------+-------------+\n",
    "# |patientID|   name|dateOfBirth|lastVisitDate|\n",
    "# +---------+-------+-----------+-------------+\n",
    "# |     1001|Ah Teck| 1991-12-31|   2012-01-20|\n",
    "# |     1002|  Kumar| 2011-10-29|   2012-09-20|\n",
    "# |     1003|    Ali| 2011-01-30|   2012-10-21|\n",
    "# +---------+-------+-----------+-------------+\n",
    "some.registerTempTable(\"some_tbl\")\n",
    "spark.sql(\"\"\"SELECT * FROM some_tbl \n",
    "WHERE TO_DATE(CAST(UNIX_TIMESTAMP(lastVisitDate, 'yyyy-MM-dd') AS TIMESTAMP)) \n",
    "BETWEEN '2012-10-15' AND current_timestamp() ORDER BY lastVisitDate\"\"\").show()\n",
    "# +---------+----+-----------+-------------+\n",
    "# |patientID|name|dateOfBirth|lastVisitDate|\n",
    "# +---------+----+-----------+-------------+\n",
    "# |     1003| Ali| 2011-01-30|   2012-10-21|\n",
    "# +---------+----+-----------+-------------+\n",
    "spark.sql(\"\"\"SELECT * FROM some_tbl \n",
    "WHERE YEAR(TO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, 'yyyy-MM-dd') AS TIMESTAMP))) = 2011\"\"\").show()\n",
    "# +---------+-----+-----------+-------------+\n",
    "# |patientID| name|dateOfBirth|lastVisitDate|\n",
    "# +---------+-----+-----------+-------------+\n",
    "# |     1002|Kumar| 2011-10-29|   2012-09-20|\n",
    "# |     1003|  Ali| 2011-01-30|   2012-10-21|\n",
    "# +---------+-----+-----------+-------------+\n",
    "spark.sql(\"\"\"SELECT name, dateOfBirth, \n",
    "                datediff(current_date(), TO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, 'yyyy-MM-dd') AS TIMESTAMP)))/365 AS age FROM some_tbl\"\"\").\n",
    "        show()\n",
    "# +-------+-----------+------------------+\n",
    "# |   name|dateOfBirth|               age|\n",
    "# +-------+-----------+------------------+\n",
    "# |Ah Teck| 1991-12-31|28.786301369863015|\n",
    "# |  Kumar| 2011-10-29| 8.945205479452055|\n",
    "# |    Ali| 2011-01-30|  9.69041095890411|\n",
    "# +-------+-----------+------------------+\n",
    "spark.sql(\"\"\"SELECT name, lastVisitDate FROM some_tbl \n",
    "WHERE datediff(current_date(), TO_DATE(CAST(UNIX_TIMESTAMP(lastVisitDate, 'yyyy-MM-dd') AS TIMESTAMP))) > 3000\"\"\").\n",
    "show()\n",
    "# +-------+-------------+\n",
    "# |   name|lastVisitDate|\n",
    "# +-------+-------------+\n",
    "# |Ah Teck|   2012-01-20|\n",
    "# +-------+-------------+\n",
    "spark.sql(\"\"\"SELECT * FROM some_tbl\n",
    "WHERE datediff(current_date(), TO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, \"yyyy-MM-dd\") AS TIMESTAMP))) > 18\"\"\").show()\n",
    "SELECT' FROM patients WHERE TO_DATE(CAST(UNIXJTlMESTAMP(dateOfBirth,\n",
    "'yyyy-MM-dd') AS TIMESTAMP}) > DATE_SUB(current_date(),INTERVAL 18 YEAR); val results = sqlContext.sql(......SELECT' FROM patients WHERE\n",
    "TO_DATE(CAST(UNIX_TIMESTAMP(dateOfBirth, 'yyyy-MM--dd') AS TIMESTAMP)) >\n",
    "DATE_SUB(current_date(), T8*365)......);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 91\n",
    "You have been given data in json format as below:\n",
    "\n",
    "{\"first_name\":\"Ankit\", \"last_name\":\"Jain\"}\n",
    "{\"first_name\":\"Amir\", \"last_name\":\"Khan\"}\n",
    "{\"first_name\":\"Rajesh\", \"last_name\":\"Khanna\"}\n",
    "{\"first_name\":\"Priynka\", \"last_name\":\"Chopra\"}\n",
    "{\"first_name\":\"Kareena\", \"last_name\":\"Kapoor\"}\n",
    "{\"first_name\":\"Lokesh\", \"last_name\":\"Yadav\"}\n",
    "\n",
    "Do the following activities:\n",
    "1. create employee.json tile locally.\n",
    "2. Load this tile on hdfs\n",
    "3. Register this data as a temp table in Spark using Python.\n",
    "4. Write select query and print this data.\n",
    "5. Now save back this selected data in json format.\n",
    "##############################################################################################\n",
    "vi employee.json\n",
    "    # Paste text\n",
    ":wq\n",
    "\n",
    "hadoop fs -put 2.json\n",
    "\n",
    "a = spark.read.json(\"/Users/dbratu/Documents/big_data/employee.json\")\n",
    "a.createTempView(\"tbl\")\n",
    "b = spark.sql(\"select * from tbl where last_name like 'K%'\").show()\n",
    "b.write.json('2b.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem Scenario 95\n",
    "You have to run your Spark application on yarn with each executor\n",
    "Maximum heap size to be 512MB and Number of processor cores to allocate on each \n",
    "executor will be 1 and Your main application required three values as input arguments\n",
    "V1 V2 V3.\n",
    "\n",
    "Please replace XXX, YYY, ZZZ -\n",
    "./bin/spark-submit -class com.hadoopexam.MyTask --master yarn-cluster--num-executors 3\n",
    "--driver-memory 512m XXX YYY lib/hadoopexam.jarZZZ\n",
    "##############################################################################################\n",
    "./bin/spark-submit \n",
    "        --class com.hadoopexam.MyTask \n",
    "        --master yarn-cluster \n",
    "        --num-executors 3\n",
    "        --driver-memory 512M\n",
    "        --executor-memory 512M\n",
    "        --executor-cores 1 \n",
    "        lib/hadoopexam.jar V1 V2 V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24 : You have been given below comma separated employee information.\n",
    "name,salary,sex,age\n",
    "alok,100000,male,29\n",
    "jatin,105000,male,32\n",
    "yogesh,134000,male,39\n",
    "ragini,112000,female,35\n",
    "jyotsana,129000,female,39\n",
    "valmiki,123000,male,29\n",
    "\n",
    "Use the netcat service on port 44444, and nc above data line by line. \n",
    "Please do the following activities.\n",
    "1. Create a flume conf file using fastest channel, which write data in hive warehouse\n",
    "directory, in a table called flumemaleemployee (Create hive table as well tor given data).\n",
    "2. While importing, make sure only male employee data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "27 : You need to implement near real time solutions for collecting information \n",
    "    when submitted in file with below information.\n",
    "\n",
    "echo \"IBM,100,20160104\" >> /tmp/spooldir/bb/.bb.txt\n",
    "echo \"IBM,103,20160105\" >> /tmp/spooldir/bb/.bb.txt\n",
    "mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt\n",
    "\n",
    "echo \"IBM,100.2,20160104\" >> /tmp/spooldir/dr/.dr.txt\n",
    "echo \"IBM,103.1,20160105\" >> /tmp/spooldir/dr/.dr.txt\n",
    "mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt\n",
    "\n",
    "You have been given below directory location (if not available than create it) /tmp/spooldir .\n",
    "You have a finacial subscription for getting stock prices from BloomBerg as well as\n",
    "Reuters and using ftp you download every hour new files from their respective ftp site in directories /tmp/spooldir/bb and /tmp/spooldir/dr respectively.\n",
    "As soon as file committed in this directory that needs to be available in hdfs in\n",
    "/tmp/flume/finance location in a single directory.\n",
    "Write a flume configuration file named flume7.conf and use it to load data in hdfs with following additional properties .\n",
    "1. Spool /tmp/spooldir/bb and /tmp/spooldir/dr\n",
    "2. File prefix in hdfs sholuld be events\n",
    "3. File suffix should be .log\n",
    "4. If file is not commited and in use than it should have _ as prefix.\n",
    "5. Data should be written as text to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "57 : You have been given below code snippet. \n",
    "    val a = sc.parallelize(1 to 9, 3) operationl\n",
    "Write a correct code snippet for operationl which will produce:\n",
    "\n",
    "Array[(String, Seq[lnt])] = Array((even,ArrayBuffer(2, 4, G, 8)), (odd,ArrayBuffer(1, 3, 5, 7,\n",
    "9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(range(1, 10))\n",
    "a.groupBy(lambda x: \"even\" if x%2 == 0 else \"odd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "34 : You have given a file named spark6/user.csv.\n",
    "id,topic,hits\n",
    "Rahul,scala,120 -\n",
    "Nikita,spark,80 -\n",
    "Mithun,spark,1 -\n",
    "myself,cca175,180\n",
    "\n",
    "Now write a Spark code in scala which will remove the header part and create RDD of values \n",
    "as below, for all rows. And also if id is myself\" than filter out row.\n",
    "Map(id -> om, topic -> scala, hits -> 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"app\", master=\"local\")\n",
    "\n",
    "dummy = \"\"\"id,topic,hits\n",
    "Rahul,scala,120 -\n",
    "Nikita,spark,80 -\n",
    "Mithun,spark,1 -\n",
    "myself,cca175,180\"\"\"\n",
    "\n",
    "a = sc.parallelize(dummy.splitlines())\n",
    "a_m = a.map(lambda x: x.split(\",\"))\n",
    "header = a_m.first()\n",
    "a_m_f = a_m.filter(lambda x: (x[0] != header[0]) and (x[0] != 'myself'))\n",
    "print(*a_m_f.collect(), sep=\"\\n\")\n",
    "a_m_f.saveAsTextFile(\"dummy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "91 : You have been given data in json format as below.\n",
    "{\"first_name\":\"Ankit\", \"last_name\":\"Jain\"}\n",
    "{\"first_name\":\"Amir\", \"last_name\":\"Khan\"}\n",
    "{\"first_name\":\"Rajesh\", \"last_name\":\"Khanna\"}\n",
    "{\"first_name\":\"Priynka\", \"last_name\":\"Chopra\"}\n",
    "{\"first_name\":\"Kareena\", \"last_name\":\"Kapoor\"}\n",
    "{\"first_name\":\"Lokesh\", \"last_name\":\"Yadav\"}\n",
    "\n",
    "Do the following activity -\n",
    "1. create employee.json tile locally.\n",
    "2. Load this tile on hdfs\n",
    "3. Register this data as a temp table in Spark using Python.\n",
    "4. Write select query and print this data.\n",
    "5. Now save back this selected data in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = spark.read.json(\"file.json\")\n",
    "content.createTempView(\"tabl\")\n",
    "data = spark.sql(\"select last_name from tabl limit 3\")\n",
    "data.write.json(\"onl3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee = spark.read.json(\"file.json\")\n",
    "employee.write.parquet(\"parq.json\")\n",
    "parq_data = spark.read.parquet(\"parq.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 :There is a parent organization called \"ABC Group Inc\", which has two child companies \n",
    "    named Tech Inc and MPTech.\n",
    "Both companies employee information is given in two separate text file as below. \n",
    "\n",
    "Please do the following activity for employee details.\n",
    "Tech Inc.txt -\n",
    "1,Alok,Hyderabad\n",
    "2,Krish,Hongkong\n",
    "3,Jyoti,Mumbai\n",
    "4,Atul,Banglore\n",
    "5,Ishan,Gurgaon\n",
    "\n",
    "MPTech.txt -\n",
    "6,John,Newyork\n",
    "7,alp2004,California\n",
    "8,tellme,Mumbai\n",
    "9,Gagan21,Pune\n",
    "10,Mukesh,Chennai\n",
    "\n",
    "1. Which command will you use to check all the available command line options on HDFS \n",
    "and How will you get the Help for individual command.\n",
    "2. Create a new Empty Directory named Employee using Command line. And also create an \n",
    "empty file named in it Techinc.txt\n",
    "3. Load both companies Employee data in Employee directory (How to override existing file \n",
    "in HDFS).\n",
    "4. Merge both the Employees data in a Single tile called MergedEmployee.txt, merged tiles \n",
    "should have new line character at the end of each file content.\n",
    "5. Upload merged file on HDFS and change the file permission on HDFS merged file, so that \n",
    "owner and group member can read and write, other user can read the file.\n",
    "6. Write a command to export the individual file as well as entire directory from HDFS to \n",
    "local file System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeing files with help of hdfs dfs command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9 : You have been given a mysql database \n",
    "    user=retail_dba \n",
    "    password=cloudera \n",
    "    database=retail_db \n",
    "    jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following.\n",
    "1. Import departments table in a directory.\n",
    "2. Again import departments table same directory \n",
    "(However, directory already exist hence it should not overrride and append the results)\n",
    "3. Also make sure your results fields are terminated by '|' and lines terminated by '\\n\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(appName=\"app\", master=\"local\")\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    appName(\"app\").\\\n",
    "    master('local').\\\n",
    "    getOrCreate()\n",
    "\n",
    "departments = spark.\\\n",
    "    read.\\\n",
    "    format(\"jdbc\").\\\n",
    "    options(url=\"jdbc:mysql://quickstart:3306/retail_db\",\n",
    "            driver = \"com.mysql.jdbc.Driver\",\n",
    "            dbtable = \"departments\",\n",
    "            user=\"retail_dba\",\n",
    "            password=\"cloudera\").\\\n",
    "    load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "68 : You have given a file as below.\n",
    "spark75/file1.txt\n",
    "File contain some text:\n",
    "\n",
    "\"\"\"Apache Hadoop is an open-source software framework written in Java for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common and should be automatically handled by the framework\n",
    "The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File\n",
    "System (HDFS) and a processing part called MapReduce. Hadoop splits files into large blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers packaged code for nodes to process in parallel based on the data that needs to be processed. his approach takes advantage of data locality nodes manipulating the data they have access to to allow the dataset to be processed faster and more efficiently than it would be in a more conventional supercomputer architecture that relies on a parallel file system where computation and data are distributed via high-speed networking\n",
    "For a slightly more complicated task, lets look into splitting up sentences from our documents into word bigrams. A bigram is pair of successive tokens in some sequence.\n",
    "We will look at building bigrams from the sequences of words in each sentence, and then try to find the most frequently occuring ones.\"\"\"\n",
    "\n",
    "The first problem is that values in each partition of our initial RDD describe lines \n",
    "from the file rather than sentences. Sentences may be split over multiple lines. \n",
    "The glom() RDD method is used to create a single entry for each document containing the \n",
    "list of all lines, we can then join the lines up, then resplit them into sentences using \".\" \n",
    "as the separator, using flatMap so that every object in our RDD is now a sentence.\n",
    "A bigram is pair of successive tokens in some sequence. Please build bigrams from the sequences\n",
    "of\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 : You have been given below code snippet:\n",
    "a = sc.parallelize([\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"]) \n",
    "b = a.keyBy(_.length) \n",
    "c = sc.parallelize([\"dog\",\"cat\",\"gnu\",\"salmon\",\"rabbit\",\"turkey\",\"wolf\",\"bear\",\"bee\"])\n",
    "d = c.keyBy(_.length)\n",
    "\n",
    "Write a correct code snippet which will produce desired output:\n",
    "Array[(lnt, (Option[String], String))] = Array((6,(Some(salmon),salmon)),\n",
    "(6,(Some(salmon),rabbit}}, (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)),\n",
    "(6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)),\n",
    "(3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),\n",
    "(3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wo!f)),\n",
    "(4,(None,bear)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize([\"dog\", \"salmon\", \"salmon\", \"rat\", \"elephant\"])\n",
    "b = a.keyBy(lambda x: len(x))\n",
    "c = sc.parallelize([\"dog\",\"cat\",\"gnu\",\"salmon\",\"rabbit\",\"turkey\",\"wolf\",\"bear\",\"bee\"])\n",
    "d = c.keyBy(lambda x: len(x))\n",
    "print(b.collect())\n",
    "print(d.collect())\n",
    "e = b.fullOuterJoin(d)\n",
    "print(e.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "56 : You have been given below code snippet:\n",
    "a = sc.parallelize(l to 100. 3) \n",
    "\n",
    "Write a correct code snippet which will produce:\n",
    "Array [Array [Int]] = Array(Array(1, 2, 3,4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19, 20,\n",
    "21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33),\n",
    "Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
    "56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66),\n",
    "Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88,\n",
    "89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.parallelize(range(1, 100), numSlices=3)\n",
    "print(a.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "31 : You have given following two files\n",
    "1. Content.txt: Contain a huge text file containing space separated words.\n",
    "2. Remove.txt: Ignore/filter all the words given in this file (Comma Separated).\n",
    "\n",
    "Write a Spark program which reads the Content.txt file and load as an RDD, \n",
    "remove all the words from a broadcast variables (which is loaded as an RDD of words from \n",
    "Remove.txt).\n",
    "And count the occurrence of the each word and save it as a text file in HDFS.\n",
    "\n",
    "Content.txt\n",
    "Hello this is ABCTech.com\n",
    "This is TechABY.com\n",
    "Apache Spark Training\n",
    "This is Spark Learning Session\n",
    "Spark is faster than MapReduce\n",
    "\n",
    "Remove.txt\n",
    "Hello, is, this, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\"Hello this is ABCTech.com\n",
    "This is TechABY.com\n",
    "Apache Spark Training\n",
    "This is Spark Learning Session\n",
    "Spark is faster than MapReduce\"\"\"\n",
    "\n",
    "remove = \"Hello, is, this, the\"\n",
    "rem_rdd = sc.parallelize(remove.split(\",\")).map(lambda x: x.strip())\n",
    "br = sc.broadcast(rem_rdd.collect())\n",
    "\n",
    "text = sc.parallelize(s.splitlines())\n",
    "text_m = text.flatMap(lambda x: x.split(\" \"))\n",
    "text_m_f = text_m.filter(lambda x: x not in br.value)\n",
    "text_m_f_f = text_m_f.map(lambda x: (x, 1))\n",
    "counter = text_m_f_f.reduceByKey(lambda x, y: x + y)\n",
    "print(text_m_f.collect())\n",
    "print(counter.collect())\n",
    "counter.saveAsTextFile(\"dumm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3: You have been given MySQL DB with following details\n",
    "user=retail_dba \n",
    "password=cloudera \n",
    "database=retail_db \n",
    "table=retail_db.categories \n",
    "jdbc URL = jdbc:mysql://quickstart:3306/retail_db\n",
    "\n",
    "Please accomplish following activities.\n",
    "1. Import data from categories table, where category=22 (Data should be stored in categories subset)\n",
    "2. Import data from categories table, where category>22 (Data should be stored in categories_subset_2)\n",
    "3. Import data from categories table, where category between 1 and 22 (Data should be stored in categories_subset_3)\n",
    "4. While importing catagories data change the delimiter to '|' (Data should be stored in categories_subset_S)\n",
    "5. Importing data from catagories table and restrict the import to category_name,category id columns only with delimiter as '|'\n",
    "6. Add null values in the table using below SQL statement ALTER TABLE categories modify category_department_id int(11); INSERT INTO categories values\n",
    "(eO.NULL.'TESTING');\n",
    "7. Importing data from catagories table (In categories_subset_17 directory) using '|' delimiter and categoryjd between 1 and 61 and encode null values for both string and non string columns.\n",
    "8. Import entire schema retail_db in a directory categories_subset_all_tables\n",
    "\n",
    "sqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "71 : Write down a Spark script using Python, in which it read a file \"Content.txt\" \n",
    "(On hdfs) with following content.\n",
    "After that split each row as (key, value), where key is first word in line and entire line as value.\n",
    "Filter out the empty lines.\n",
    "And save this key value in \"problem86\" as Sequence file(On hdfs)\n",
    "Save as sequence file , where key as null and entire line as value. \n",
    "Read back the stored sequence files.\n",
    "\n",
    "Content.txt -\n",
    "\n",
    "Hello this is ABCTECH.com -\n",
    "\n",
    "This is XYZTECH.com -\n",
    "\n",
    "Apache Spark Training -\n",
    "\n",
    "This is Spark Learning Session -\n",
    "\n",
    "Spark is faster than MapReduce -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"app\", master='local')\n",
    "\n",
    "a = \"\"\"Hello this is ABCTECH.com -\n",
    "\n",
    "This is XYZTECH.com -\n",
    "\n",
    "Apache Spark Training -\n",
    "\n",
    "This is Spark Learning Session -\n",
    "\n",
    "Spark is faster than MapReduce -\"\"\"\n",
    "\n",
    "txt = sc.parallelize(a.splitlines())\n",
    "txt_m = txt.map(lambda x: (x.split(\" \")[0], x))\n",
    "txt_m_f = txt_m.filter(lambda x: x[1] != \"\")\n",
    "print(txt_m_f.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "45 : You have been given 2 files , with the content as given Below\n",
    "(spark12/technology.txt)\n",
    "first,last,technology\n",
    "\n",
    "Amit,Jain,java -\n",
    "\n",
    "Lokesh,kumar,unix -\n",
    "\n",
    "Mithun,kale,spark -\n",
    "\n",
    "Rajni,vekat,hadoop -\n",
    "\n",
    "Rahul,Yadav,scala -\n",
    "\n",
    "(spark12/salary.txt)\n",
    "first,last,salary\n",
    "\n",
    "Amit,Jain,100000 -\n",
    "\n",
    "Lokesh,kumar,95000 -\n",
    "\n",
    "Mithun,kale,150000 -\n",
    "\n",
    "Rajni,vekat,154000 -\n",
    "\n",
    "Rahul,Yadav,120000 -\n",
    "\n",
    "Write a Spark program, which will join the data based on first and last name and \n",
    "save the joined results in following format, first Last.technology.salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"app\", master='local')\n",
    "\n",
    "technology = \"\"\"\n",
    "first,last,technology\n",
    "\n",
    "Amit,Jain,java -\n",
    "\n",
    "Lokesh,kumar,unix -\n",
    "\n",
    "Mithun,kale,spark -\n",
    "\n",
    "Rajni,vekat,hadoop -\n",
    "\n",
    "Rahul,Yadav,scala -\"\"\"\n",
    "\n",
    "salary = \"\"\"first,last,salary\n",
    "\n",
    "Amit,Jain,100000 -\n",
    "\n",
    "Lokesh,kumar,95000 -\n",
    "\n",
    "Mithun,kale,150000 -\n",
    "\n",
    "Rajni,vekat,154000 -\n",
    "\n",
    "Rahul,Yadav,120000 -\"\"\"\n",
    "\n",
    "# join the data based on first and last name and save as\n",
    "# first, last, technology, salary\n",
    "\n",
    "technology_rdd = sc.\\\n",
    "    parallelize(technology.splitlines()).\\\n",
    "    filter(lambda x: len(x) > 0).\\\n",
    "    map(lambda x: ((x.split(\",\")[0], x.split(\",\")[1]), x.split(\",\")[2])).\\\n",
    "    filter(lambda x: x[0][0] != 'first')\n",
    "salary_rdd = sc.\\\n",
    "    parallelize(salary.splitlines()).\\\n",
    "    filter(lambda x: len(x) > 0).\\\n",
    "    map(lambda x: ((x.split(\",\")[0], x.split(\",\")[1]), x.split(\",\")[2].strip(\"-\").strip(\" \"))).\\\n",
    "    filter(lambda x: x[0][0] != 'first')\n",
    "\n",
    "joined = technology_rdd.join(salary_rdd)\n",
    "print(technology_rdd.collect())\n",
    "print(salary_rdd.collect())\n",
    "print(joined.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "54 : You have been given below code snippet\n",
    "a = sc.parallelize([\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"]) \n",
    "b = a.map(x => (x.length, x))\n",
    "\n",
    "Write a correct code snippet which will produce desired output, shown below.\n",
    "Array[(lnt, String)] = Array((4,lion), (7,panther), (3,dogcat), (5,tigereagle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"aoo\", master=\"local\")\n",
    "\n",
    "a = sc.parallelize([\"dog\", \"tiger\", \"lion\", \"cat\", \"panther\", \"eagle\"])\n",
    "b = a.map(lambda x: (len(x), x))\n",
    "print(a.collect())\n",
    "print(b.collect())\n",
    "\n",
    "c = b.reduceByKey(lambda x, y: x + y)\n",
    "print(c.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "52 : You have been given below code snippet.\n",
    "b = sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1])\n",
    "\n",
    "Write a correct code snippet for Operation_xyz which will produce below output.\n",
    "(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> S, 2 -> 3, 4 -> 2, 7 ->1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"aoo\", master=\"local\")\n",
    "\n",
    "b = sc.parallelize([1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1])\n",
    "\n",
    "print(b.countByValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "81 : You have been given following product.csv\n",
    "productID,productCode,name,quantity,price\n",
    "1001,PEN,Pen Red,5000,1.23\n",
    "1002,PEN,Pen Blue,8000,1.25\n",
    "1003,PEN,Pen Black,2000,1.25\n",
    "1004,PEC,Pencil 2B,10000,0.48\n",
    "1005,PEC,Pencil 2H,8000,0.49\n",
    "1006,PEC,Pencil HB,0,9999.99\n",
    "\n",
    "1. Create a Hive ORC table using SparkSql\n",
    "2. Load this data in Hive table.\n",
    "3. Create a Hive parquet table using SparkSQL and load data in it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
